{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHjGTJqEvBPv"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class GitHubScraper:\n",
        "    def __init__(self, token: str):\n",
        "        \"\"\"\n",
        "        Initialize the GitHub scraper with your API token.\n",
        "\n",
        "        Args:\n",
        "            token (str): GitHub Personal Access Token\n",
        "        \"\"\"\n",
        "        self.headers = {\n",
        "            'Authorization': f'token {token}',\n",
        "            'Accept': 'application/vnd.github.v3+json'\n",
        "        }\n",
        "        self.base_url = 'https://github.com/[login]/[repository]'\n",
        "\n",
        "        # Setup logging\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def _make_request(self, url: str, params: dict = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Make a request to the GitHub API with rate limit handling.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            response = requests.get(url, headers=self.headers, params=params)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            elif response.status_code == 403:\n",
        "                reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n",
        "                sleep_time = max(reset_time - time.time(), 0) + 1\n",
        "                self.logger.warning(f\"Rate limit hit. Sleeping for {sleep_time} seconds\")\n",
        "                time.sleep(sleep_time)\n",
        "            else:\n",
        "                self.logger.error(f\"Error {response.status_code}: {response.text}\")\n",
        "                response.raise_for_status()\n",
        "\n",
        "    def clean_company_name(self, company: str) -> str:\n",
        "        \"\"\"\n",
        "        Clean up company names according to specifications.\n",
        "        \"\"\"\n",
        "        if not company:\n",
        "            return \"\"\n",
        "\n",
        "        # Strip whitespace and @ symbol\n",
        "        cleaned = company.strip().lstrip('@')\n",
        "\n",
        "        # Convert to uppercase\n",
        "        return cleaned.upper()\n",
        "\n",
        "    def search_users(self, location: str, min_followers: int) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Search for GitHub users in a specific location with minimum followers.\n",
        "        \"\"\"\n",
        "        users = []\n",
        "        page = 1\n",
        "\n",
        "        while True:\n",
        "            self.logger.info(f\"Fetching users page {page}\")\n",
        "\n",
        "            query = f\"location:{location} followers:>={min_followers}\"\n",
        "            params = {\n",
        "                'q': query,\n",
        "                'per_page': 100,\n",
        "                'page': page\n",
        "            }\n",
        "\n",
        "            url = f\"{self.base_url}/search/users\"\n",
        "            response = self._make_request(url, params)\n",
        "\n",
        "            if not response['items']:\n",
        "                break\n",
        "\n",
        "            for user in response['items']:\n",
        "                user_data = self._make_request(user['url'])\n",
        "\n",
        "                # Extract only the required fields with exact matching names\n",
        "                cleaned_data = {\n",
        "                    'login': user_data['login'],\n",
        "                    'name': user_data['name'] if user_data['name'] else \"\",\n",
        "                    'company': self.clean_company_name(user_data.get('company')),\n",
        "                    'location': user_data['location'] if user_data['location'] else \"\",\n",
        "                    'email': user_data['email'] if user_data['email'] else \"\",\n",
        "                    'hireable': user_data['hireable'] if user_data['hireable'] is not None else False,\n",
        "                    'bio': user_data['bio'] if user_data['bio'] else \"\",\n",
        "                    'public_repos': user_data['public_repos'],\n",
        "                    'followers': user_data['followers'],\n",
        "                    'following': user_data['following'],\n",
        "                    'created_at': user_data['created_at']\n",
        "                }\n",
        "\n",
        "                users.append(cleaned_data)\n",
        "\n",
        "            page += 1\n",
        "\n",
        "        return users\n",
        "\n",
        "    def get_user_repositories(self, username: str, max_repos: int = 500) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Get repositories for a specific user.\n",
        "        \"\"\"\n",
        "        repos = []\n",
        "        page = 1\n",
        "\n",
        "        while len(repos) < max_repos:\n",
        "            self.logger.info(f\"Fetching repositories for {username}, page {page}\")\n",
        "\n",
        "            params = {\n",
        "                'sort': 'pushed',\n",
        "                'direction': 'desc',\n",
        "                'per_page': 100,\n",
        "                'page': page\n",
        "            }\n",
        "\n",
        "            url = f\"{self.base_url}/users/{username}/repos\"\n",
        "            response = self._make_request(url, params)\n",
        "\n",
        "            if not response:\n",
        "                break\n",
        "\n",
        "            for repo in response:\n",
        "                # Extract only the required fields with exact matching names\n",
        "                repo_data = {\n",
        "                    'login': username,  # Adding owner's login as required\n",
        "                    'full_name': repo['full_name'],\n",
        "                    'created_at': repo['created_at'],\n",
        "                    'stargazers_count': repo['stargazers_count'],\n",
        "                    'watchers_count': repo['watchers_count'],\n",
        "                    'language': repo['language'] if repo['language'] else \"\",\n",
        "                    'has_projects': repo['has_projects'],\n",
        "                    'has_wiki': repo['has_wiki'],\n",
        "                    'license_name': repo['license']['key'] if repo.get('license') else \"\"\n",
        "                }\n",
        "\n",
        "                repos.append(repo_data)\n",
        "\n",
        "            if len(response) < 100:\n",
        "                break\n",
        "\n",
        "            page += 1\n",
        "\n",
        "        return repos[:max_repos]\n",
        "\n",
        "def main():\n",
        "    # Get GitHub token\n",
        "    token = input(\"Enter your GitHub token: \").strip()\n",
        "    if not token:\n",
        "        print(\"Token is required. Exiting...\")\n",
        "        return\n",
        "\n",
        "    # Initialize scraper\n",
        "    scraper = GitHubScraper(token)\n",
        "\n",
        "    # Search for users in Delhi with >100 followers\n",
        "    users = scraper.search_users(location='Hyderabad', min_followers=50)\n",
        "\n",
        "    # Save users to CSV\n",
        "    users_df = pd.DataFrame(users)\n",
        "    users_df.to_csv('users.csv', index=False)\n",
        "\n",
        "    # Get repositories for each user\n",
        "    all_repos = []\n",
        "    for user in users:\n",
        "        repos = scraper.get_user_repositories(user['login'])\n",
        "        all_repos.extend(repos)\n",
        "\n",
        "    # Save repositories to CSV\n",
        "    repos_df = pd.DataFrame(all_repos)\n",
        "    repos_df.to_csv('repositories.csv', index=False)\n",
        "\n",
        "    print(f\"Scraped {len(users)} users and {len(all_repos)} repositories\")\n",
        "\n",
        "    # Create README.md\n",
        "    with open('README.md', 'w') as f:\n",
        "        f.write(f\"\"\"# Project: GitHub User Scraping for Hyderabad

          - **Data Scraping**: Using the GitHub API, we gathered data on users in Hyderabad with over 50 followers, along with their repositories.
          - **Interesting Finding**: After analysis, we found that JavaScript is the most popular language among Hyderabad's top developers.
          
          ## Overview
          
          This project collects and analyzes GitHub data for users based in Hyderabad with over 50 followers. By examining user profiles and repository attributes, we aim to uncover trends in programming language preferences, license popularity, and other factors that can guide local developers in building their GitHub presence.
          
          ## Data Collection
          
          Using GitHub's API, we retrieved profiles of users in Hyderabad with follower counts exceeding 50. For each user, we collected up to 500 of their most recently pushed repositories, capturing essential details such as programming language, stargazers, watchers, and license type.
          
          ### Files in This Repository
          
          - **users.csv**: Contains data on each GitHub user from Hyderabad with over 50 followers. Fields include:
            - `login`: GitHub user ID
            - `name`: Full name
            - `company`: Company name (cleaned to uppercase and leading `@` symbol removed)
            - `location`: City of residence
            - `email`: Email address
            - `hireable`: Open to hiring status
            - `bio`: Short biography
            - `public_repos`: Number of public repositories
            - `followers`: Follower count
            - `following`: Following count
            - `created_at`: Account creation date
          
          - **repositories.csv**: Lists public repositories of each user in `users.csv`. Fields include:
            - `login`: GitHub user ID of repository owner
            - `full_name`: Repository name
            - `created_at`: Repository creation date
            - `stargazers_count`: Star count
            - `watchers_count`: Watcher count
            - `language`: Repository programming language
            - `has_projects`: Projects enabled status
            - `license_name`: Repository license type
          
          - **README.md**: Documentation outlining data collection methods, insights from the analysis, and actionable recommendations for developers.
          
          ## Analysis and Findings
          
          After analyzing the data, several key insights emerged:
          
          1. **Programming Language Preference**: JavaScript is the most popular language among Hyderabad’s most-followed developers.
          2. **License Popularity**: The most common licenses among Hyderabad developers are MIT, Apache-2.0, and others.
          3. **Community Engagement**: Many prominent developers in Hyderabad actively contribute to open-source projects.
          
          ## Recommendations for Developers
          
          Developers in Hyderabad looking to improve their GitHub presence may benefit from contributing to popular open-source projects, especially those in languages widely used by the local developer community. Engaging with such projects can increase visibility and foster connections with other professionals in the area.
,
        "\"\"\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "users=pd.read_csv('/content/users.csv')\n",
        "users"
      ],
      "metadata": {
        "id": "t0bgJr1c8Zzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1. Who are the top 5 users in Hyderabad with the highest number of followers? List their login in order, comma-separated.\n",
        "# Users\n",
        "\n",
        "# sort users by followers in descending order and take the top 5\n",
        "top_5_users = users.sort_values('followers', ascending=False).head(5)\n",
        "\n",
        "# Extract logins and join them with commas\n",
        "top_5_logins = ','.join(top_5_users['login'].tolist())\n",
        "\n",
        "top_5_logins"
      ],
      "metadata": {
        "id": "OAI55HrW82Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: # Q2. Who are the 5 earliest registered GitHub users in Hyderabad? List their login in ascending order of created_at, comma-separated.\n",
        "# # Users\n",
        "\n",
        "# Sort users by created_at in ascending order and take the top 5\n",
        "earliest_5_users = users.sort_values('created_at').head(5)\n",
        "\n",
        "# Extract logins and join them with commas\n",
        "earliest_5_logins = ','.join(earliest_5_users['login'].tolist())\n",
        "\n",
        "earliest_5_logins"
      ],
      "metadata": {
        "id": "CQdTYq8O9C6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q3. What are the 3 most popular license among these users? Ignore missing licenses. List the license_name in order, comma-separated.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "repos_df = pd.read_csv('repositories.csv')\n",
        "\n",
        "# Remove rows with missing license_name\n",
        "repos_df = repos_df.dropna(subset=['license_name'])\n",
        "\n",
        "# Count the occurrences of each license_name\n",
        "license_counts = repos_df['license_name'].value_counts()\n",
        "\n",
        "# Get the top 3 most popular licenses\n",
        "top_3_licenses = license_counts.head(3).index.tolist()\n",
        "\n",
        "# Convert the list to a comma-separated string\n",
        "print(','.join(top_3_licenses))"
      ],
      "metadata": {
        "id": "tVVfiOUD-Pzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q4.  Which company do the majority of these developers work at?\n",
        "# Company (cleaned up as explained above)\n",
        "\n",
        "company_counts = users['company'].value_counts()\n",
        "most_frequent_company = company_counts.idxmax()\n",
        "most_frequent_company"
      ],
      "metadata": {
        "id": "jzElhPln-nuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repos_df = pd.read_csv('/content/repositories.csv')"
      ],
      "metadata": {
        "id": "CZGQBRZo_2Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q5. Which programming language is most popular among these users?\n",
        "# Language\n",
        "\n",
        "\n",
        "\n",
        "# Group by language and count the number of repositories for each language\n",
        "language_counts = repos_df.groupby('language')['language'].count()\n",
        "\n",
        "# Find the language with the most repositories\n",
        "most_popular_language = language_counts.idxmax()\n",
        "\n",
        "most_popular_language"
      ],
      "metadata": {
        "id": "HaOhBH9b-zed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q6. Which programming language is the second most popular among users who joined after 2020?\n",
        "# Language\n",
        "\n",
        "users_after_2020 = users[users['created_at'] > '2020-01-01']\n",
        "repos_2020 = repos_df[repos_df['login'].isin(users_after_2020['login'].tolist())]\n",
        "repos_2020['language'].value_counts().head()"
      ],
      "metadata": {
        "id": "AizEBu_Y_DXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q7. Which language has the highest average number of stars per repository?\n",
        "# Language\n",
        "\n",
        "# Calculate the average number of stars per repository for each language\n",
        "avg_stars_per_language = repos_df.groupby('language')['stargazers_count'].mean()\n",
        "\n",
        "# Find the language with the highest average number of stars\n",
        "language_with_highest_avg_stars = avg_stars_per_language.idxmax()\n",
        "\n",
        "language_with_highest_avg_stars"
      ],
      "metadata": {
        "id": "F1QBVAy4BnBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q8. Let's define leader_strength as followers / (1 + following). Who are the top 5 in terms of leader_strength? List their login in order, comma-separated.\n",
        "# User login\n",
        "\n",
        "# Calculate leader_strength for each user\n",
        "users['leader_strength'] = users['followers'] / (1 + users['following'])\n",
        "\n",
        "# Sort users by leader_strength in descending order and take the top 5\n",
        "top_5_leader_strength = users.sort_values('leader_strength', ascending=False).head(5)\n",
        "\n",
        "# Extract logins and join them with commas\n",
        "top_5_leader_logins = ','.join(top_5_leader_strength['login'].tolist())\n",
        "\n",
        "top_5_leader_logins"
      ],
      "metadata": {
        "id": "2izB95XKB8cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q9. What is the correlation between the number of followers and the number of public repositories among users in Hyderabad?\n",
        "# Correlation between followers and repos (to 3 decimal places, e.g. 0.123 or -\n",
        "\n",
        "# Calculate the correlation between followers and public repositories\n",
        "correlation = users['followers'].corr(users['public_repos'])\n",
        "\n",
        "# Print the correlation rounded to 3 decimal places\n",
        "print(round(correlation, 3))"
      ],
      "metadata": {
        "id": "4tJ9RHjuCEyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q10. Does creating more repos help users get more followers? Using regression, estimate how many additional followers a user gets per additional public repository.\n",
        "# Regression slope of followers on repos (to 3 decimal places, e.g. 0.123 or -0.123)\n",
        "\n",
        "import statsmodels.formula.api as sm\n",
        "\n",
        "# Fit a linear regression model\n",
        "model = sm.ols('followers ~ public_repos', data=users).fit()\n",
        "\n",
        "# Extract the slope coefficient\n",
        "slope = model.params['public_repos']\n",
        "\n",
        "# Print the slope rounded to 3 decimal places\n",
        "print(round(slope, 3))"
      ],
      "metadata": {
        "id": "63rm6IVpCgTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q11. Do people typically enable projects and wikis together? What is the correlation between a repo having projects enabled and having wiki enabled?\n",
        "# Correlation between projects and wiki enabled (to 3 decimal places, e.g. 0.123 or -0.123)\n",
        "\n",
        "correlation = repos_df['has_projects'].corr(repos_df['has_wiki'])\n",
        "print(round(correlation, 3))"
      ],
      "metadata": {
        "id": "XPjqtJT6D4QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q12. Do hireable users follow more people than those who are not hireable?\n",
        "# Average of following per user for hireable=true minus the average following for the rest (to 3 decimal places, e.g. 12.345 or -12.345)\n",
        "\n",
        "avg_following_hireable = users[users['hireable'] == True]['following'].mean()\n",
        "avg_following_not_hireable = users[users['hireable'] == False]['following'].mean()\n",
        "difference = avg_following_hireable - avg_following_not_hireable\n",
        "print(round(difference, 3))"
      ],
      "metadata": {
        "id": "dUk0aqQfEChg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q13. Some developers write long bios. Does that help them get more followers? What's the correlation of the length of their bio (in Unicode words, split by whitespace) with followers? (Ignore people without bios)\n",
        "# Regression slope of followers on bio word count (to 3 decimal places, e.g. 12.345 or -12.345)\n",
        "\n",
        "# Remove rows with missing bios\n",
        "users_with_bios = users.dropna(subset=['bio'])\n",
        "\n",
        "# Calculate the length of the bio in Unicode words\n",
        "users_with_bios['bio_word_count'] = users_with_bios['bio'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# Fit a linear regression model\n",
        "model = sm.ols('followers ~ bio_word_count', data=users_with_bios).fit()\n",
        "\n",
        "# Extract the slope coefficient\n",
        "slope = model.params['bio_word_count']\n",
        "\n",
        "# Print the slope rounded to 3 decimal places\n",
        "print(round(slope, 3))"
      ],
      "metadata": {
        "id": "U5krJ2avJyJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q14. Who created the most repositories on weekends (UTC)? List the top 5 users' login in order, comma-separated\n",
        "# Users login\n",
        "\n",
        "# Convert 'created_at' column to datetime objects\n",
        "repos_df['created_at'] = pd.to_datetime(repos_df['created_at'])\n",
        "\n",
        "# Extract the day of the week (0 = Monday, 6 = Sunday)\n",
        "repos_df['day_of_week'] = repos_df['created_at'].dt.dayofweek\n",
        "\n",
        "# Filter for weekend repositories (Saturday and Sunday)\n",
        "weekend_repos = repos_df[repos_df['day_of_week'].isin([5, 6])]\n",
        "\n",
        "# Count the number of weekend repositories created by each user\n",
        "weekend_repo_counts = weekend_repos.groupby('login')['login'].count()\n",
        "\n",
        "# Sort users by the number of weekend repositories in descending order\n",
        "top_users = weekend_repo_counts.sort_values(ascending=False).head(5)\n",
        "\n",
        "# Extract the top 5 users' logins and join them with commas\n",
        "top_5_weekend_users = ','.join(top_users.index.tolist())\n",
        "\n",
        "top_5_weekend_users"
      ],
      "metadata": {
        "id": "YFx99zlVK-n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q15. Do people who are hireable share their email addresses more often?\n",
        "# [fraction of users with email when hireable=true] minus [fraction of users with email for the rest] (to 3 decimal places, e.g. 0.123 or -0.123)\n",
        "\n",
        "fraction_hireable_email = users[users['hireable'] == True]['email'].notna().mean()\n",
        "fraction_not_hireable_email = users[users['hireable'] == False]['email'].notna().mean()\n",
        "difference = fraction_hireable_email - fraction_not_hireable_email\n",
        "print(round(difference, 3))"
      ],
      "metadata": {
        "id": "ynQuaBy5LOZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Q16. Let's assume that the last word in a user's name is their surname (ignore missing names, trim and split by whitespace.) What's the most common surname? (If there's a tie, list them all, comma-separated, alphabetically)\n",
        "# Number of users with the most common surname\n",
        "\n",
        "# Assuming the last word in a user's name is their surname\n",
        "def get_surname(name):\n",
        "  if isinstance(name, str):\n",
        "    name_parts = name.strip().split()\n",
        "    if name_parts:\n",
        "      return name_parts[-1]\n",
        "  return None\n",
        "\n",
        "users['surname'] = users['name'].apply(get_surname)\n",
        "\n",
        "# Count the occurrences of each surname\n",
        "surname_counts = users['surname'].value_counts()\n",
        "\n",
        "# Get the most common surname(s)\n",
        "most_common_count = surname_counts.max()\n",
        "most_common_surnames = surname_counts[surname_counts == most_common_count].index.tolist()\n",
        "\n",
        "# Sort the surnames alphabetically if there's a tie\n",
        "most_common_surnames.sort()\n",
        "\n",
        "# Print the most common surname(s)\n",
        "print(','.join(most_common_surnames))\n",
        "\n",
        "# Print the number of users with the most common surname\n",
        "most_common_count"
      ],
      "metadata": {
        "id": "HjARxHIYLs1J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
